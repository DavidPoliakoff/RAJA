.. ##
.. ## Copyright (c) 2016, Lawrence Livermore National Security, LLC.
.. ##
.. ## Produced at the Lawrence Livermore National Laboratory.
.. ##
.. ## All rights reserved.
.. ##
.. ## For release details and restrictions, please see raja/README-license.txt
.. ##


===================================
Future development
===================================

As noted in the introduction, RAJA is very much a work-in-progress.
Additional features will appear in future releases. Items currently in 
development or planned include:

  * To complement RAJA, we are developing a resource manager runtime layer 
    that moves data automatically to desired memory locations based on 
    RAJA execution contexts. So, for example, applications using RAJA will
    have option to use either Unified Memory or explicit host-to-device and
    device-to-host data transfers when using CUDA. Similarly, for using other
    forms of high-bandwidth memory on other architectures. This can be done 
    in a manner that is larger transparent to application code. Cool, huh?

  * Support for CUDA streams, which can yield a significant performance
    boost on GPU systems.

  * A cleaner implementation of IndexSet that will allow compile-time 
    selection of Segment types.  This will make it easier to add new
    Segment types by avoiding the need for switch statements in the 
    implementation. It will also help to reduce code bloat since only code
    for the Segment types needed will be generated by the compiler.

  * Other configuration improvements to make it easier for uses to select
    the sort of functionality they need at compile time; e.g., segment types,
    forall templates, etc.

  * Additional Segment and IndexSet support for nested-loops. We have some
    of this in the code now (forallN stuff), but there are other ways that 
    may be easier for different applications to use.

  * Removal of virtual inheritance in the IndexSet Segment types. This 
    prevents Segment objects from being created in host code and then
    passing them as arguments to '__global__' CUDA functions to be used
    in GPU device code. This is the main reason why we do not have a 
    fully-functional GPU version of the CoMD proxy app.

  * CUDA versions of the nested loop (forallN) functionality. Currently, the
    fact that CUDA requires the '__device__' decoration on a lambda definition,
    when the lambda will execute on the device makes it difficult to write
    generic code that will run on either a CPU or GPU. So, rather than confuse
    others, we left this out for now. 

  * A faster implementation of the CUDA reductions. We've been incrementally
    improving performance for a while and we think there is still more
    to be gained.

  * "Min-loc" and "max-loc" reduction classes for CUDA. Just haven't gotten 
    around to these yet.
 
  * Additional tests and example codes.

**Stay tuned...**
