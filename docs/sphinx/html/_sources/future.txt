.. #######################################################################
.. #
.. # Copyright (c) 2016, Lawrence Livermore National Security, LLC.
.. #
.. # Produced at the Lawrence Livermore National Laboratory.
.. #
.. # All rights reserved.
.. #
.. # This source code cannot be distributed without permission and
.. # further review from Lawrence Livermore National Laboratory.
.. #
.. #######################################################################


===================================
Future development
===================================

As noted in the introduction, RAJA is very much a work-in-progress.
Additional features will appear in future releases. Items currently in 
development or planned include:

  * To complement RAJA, we are developing a resource manager runtime layer 
    that moves data automatically to desired memory locations based on 
    RAJA execution contexts. So, for example, applications using RAJA will
    have option to use either Unified Memory or explicit host-to-device and
    device-to-host data transfers when using CUDA. Similarly, for using other
    forms of high-bandwidth memory on other architectures. This can be done 
    in a manner that is larger transparent to application code. Cool, huh?

  * Support for CUDA streams, which can yield a significant performance
    boost on GPU systems.

  * A cleaner implementation of IndexSet that will allow compile-time 
    selection of Segment types.  This will make it easier to add new
    Segment types by avoiding the need for switch statements in the 
    implementation. It will also help to reduce code bloat since only code
    for the Segment types needed will be generated by the compiler.

  * Removal of virtual inheritance in the IndexSet Segment types. This 
    prevents Segment objects from being created in host code and then
    passing them as arguments to '__global__' CUDA functions to be used
    in GPU device code. This is the main reason why we do not have a 
    fully-functional GPU version of the CoMD proxy app.

  * Additional Segment and IndexSet support for nested-loops. We have some
    of this in the code now, but there are other ways that may be easier
    for different applications to use.

  * A faster implementation of the CUDA reductions. We've been incrementally
    improving performance for a while and we think there is still more
    to be gained.

  * "Min-loc" and "max-loc" reduction classes for CUDA. Just haven't gotten 
    around to these yet.
 
  * Additional tests and example codes.

**Stay tuned...**
